#  Clasificaci√≥n con √Årbol de Decisi√≥n ‚Äì Wine Dataset

## Descripci√≥n del proyecto

Este proyecto implementa un **clasificador basado en √Årboles de Decisi√≥n** utilizando el **Wine Dataset** incluido en scikit-learn.  
El objetivo principal es:

- Entrenar un modelo de clasificaci√≥n.
- Interpretar las reglas generadas por el √°rbol.
- Analizar c√≥mo afecta el par√°metro `max_depth` al comportamiento del modelo.
- Evaluar la precisi√≥n del clasificador usando datos de prueba.

El Wine Dataset contiene **178 muestras de vino**, cada una con **13 caracter√≠sticas qu√≠micas** y una etiqueta que indica uno de **3 tipos de vino** producidos en Piamonte, Italia.

---

## Proceso del modelo

### 1. Carga del dataset

Se utiliza `load_wine()` para obtener:
- **X**: caracter√≠sticas qu√≠micas (alcohol, flavanoides, proline, etc.)
- **y**: clase del vino (0, 1 o 2)

### 2. Divisi√≥n del dataset

Mediante `train_test_split` se divide en:
- 80% datos de entrenamiento  
- 20% datos de prueba  

### 3. Entrenamiento del √°rbol de decisi√≥n

Se entren√≥ el clasificador variando el par√°metro `max_depth`:

- `max_depth = 1`
- `max_depth = 2`
- `max_depth = 3`

Para cada valor se imprimieron las reglas del √°rbol con `export_text` y se evalu√≥ la precisi√≥n del modelo en los datos de prueba.

---

## üìä Resultados obtenidos

### üîπ **max_depth = 1**

|--- color_intensity <= 3.82
| |--- class: 1
|--- color_intensity > 3.82
| |--- class: 0

**Precisi√≥n:** 0.66  

**An√°lisis:**  
- El modelo usa solo una caracter√≠stica.  
- Es demasiado simple ‚Üí bajo rendimiento.  
- Existe **subajuste (underfitting)**.

---

### üîπ **max_depth = 2**

|--- color_intensity <= 3.82
| |--- proline <= 1002.50 ‚Üí class 1
| |--- proline > 1002.50 ‚Üí class 0
|--- color_intensity > 3.82
| |--- flavanoids <= 1.40 ‚Üí class 2
| |--- flavanoids > 1.40 ‚Üí class 0

**Precisi√≥n:** 0.86  

**An√°lisis:**  
- Se utilizan m√°s caracter√≠sticas relevantes.  
- Ahora el modelo distingue bien las tres clases.  
- Buen balance entre interpretabilidad y precisi√≥n.

---

### üîπ **max_depth = 3**

El √°rbol creci√≥ m√°s e incluy√≥ caracter√≠sticas como **ash** y umbrales m√°s espec√≠ficos.

**Precisi√≥n:** 0.94 (mejor resultado)

**An√°lisis:**  
- Captura m√°s relaciones del dataset.  
- A√∫n no muestra sobreajuste evidente.  
- Las reglas siguen siendo lo suficientemente interpretables.

---

## Conclusiones sobre el par√°metro `max_depth`

- **Poca profundidad (1):**  
  - √Årbol muy simple  
  - Baja precisi√≥n (0.6666666666666666)
  - Underfitting  

- **Profundidad media (2‚Äì3):**  
  - Reglas completas  
  - Mejor desempe√±o  
  - Buen equilibrio entre simplicidad y exactitud
  - Exactitud de 2: 0.8611111111111112
  - Exactitud de 3: 0.9444444444444444 

- **Profundidad alta o sin l√≠mite (`max_depth=None`):**  
  - El √°rbol crece demasiado  
  - Las reglas se vuelven largas y dif√≠ciles de interpretar  
  - Aparece el **sobreajuste**  

---

## Opini√≥n personal sobre los resultados

Los resultados muestran claramente c√≥mo la profundidad del √°rbol afecta la calidad del modelo.  
Cuando `max_depth` es muy bajo, el modelo no aprende lo suficiente y su precisi√≥n es baja.  
Al aumentar la profundidad, el modelo mejora y las reglas se vuelven m√°s espec√≠ficas sin perder interpretabilidad.

En mis pruebas, **max_depth = 3** ofreci√≥ el mejor equilibrio entre interpretabilidad y precisi√≥n. Esto demuestra la importancia de controlar la profundidad para evitar tanto el subajuste como el sobreajuste.

---

## ¬øMi base de conocimiento puede usarse en un √°rbol de decisi√≥n?

**No. Mi proyecto actual no usa √°rboles de decisi√≥n, sino un motor l√≥gico basado en
Resoluci√≥n SLD, unificaci√≥n de variables, b√∫squeda DFS y reglas declarativas.**

Los √°rboles de decisi√≥n requieren:
- atributos num√©ricos o categ√≥ricos simples,
- condiciones tipo ‚Äúsi-entonces‚Äù fijas,
- caminos deterministas,
- datos tabulares.

Mi sistema, en cambio:
- usa predicados l√≥gicos,
- maneja variables libres,
- deduce informaci√≥n mediante encadenamiento hacia atr√°s,
- aplica unificaci√≥n y renombrado de variables,
- puede generar m√∫ltiples soluciones.

Por estas razones, **no es posible convertir la base de conocimiento directamente en un √°rbol de decisi√≥n.**

### ¬øQu√© cambios ser√≠an necesarios?

Para usar un √°rbol de decisi√≥n tendr√≠a que transformar toda la base de conocimiento en un dataset estructurado, donde cada tr√°mite sea una fila, cada caracter√≠stica sea una columna y exista una ‚Äúclase‚Äù fija a predecir. Las reglas l√≥gicas tendr√≠an que reemplazarse por atributos expl√≠citos.

### ¬øPodr√≠a usarse un modelo de regresi√≥n?

Solo si la salida deseada es num√©rica, como tiempo de tr√°mite o costo.  
La mayor√≠a de mis reglas generan categor√≠as, requisitos o decisiones l√≥gicas, por lo que la regresi√≥n **no ser√≠a adecuada en la mayor√≠a de los casos.**

